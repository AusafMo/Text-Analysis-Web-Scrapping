{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies"
      ],
      "metadata": {
        "id": "5sAqcCnOaqqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "C2aStjkvzE82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f8e4d8-9bc4-4fb4-9a2b-d2b41a60bbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the custom STOPWORD dictionary"
      ],
      "metadata": {
        "id": "LkYot7S3mwQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise a string to save the txt\n",
        "stop_string = ''\n",
        "\n",
        "dir = '/content/drive/MyDrive/Internship/StopWords'\n",
        "\n",
        "dir_cnt = 0    \n",
        "\n",
        "# preprocessing and saving the text in the directories in a list named stop_words\n",
        "for filename in os.listdir(dir) :\n",
        "\n",
        "    f = os.path.join(dir, filename)\n",
        "    with open(f, 'r', encoding = 'latin') as text_file :\n",
        "      dir_cnt += 1\n",
        "\n",
        "      content = text_file.read()\n",
        "      content = content.replace('\\n', ' ')\n",
        "      content = content.replace('|', ' ')\n",
        "\n",
        "      stop_string += content\n",
        "      stop_words = stop_string.split(' ')\n",
        "\n",
        "print(stop_words)\n",
        "print(f'The total stop words are {len(stop_words)} in total of {dir_cnt} \".txt\" files.')"
      ],
      "metadata": {
        "id": "b8b_ddwxEC3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of null Strings are {stop_words.count('')}.\")\n",
        "\n",
        "# removing any null values\n",
        "stop_words = [item for item in stop_words if item != '']\n",
        "\n",
        "print(f'The total Stop Words now is {len(stop_words)} after removing the null strings.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOBOVA1Pt2cg",
        "outputId": "75a99de9-831d-4143-e922-3c5c2af48f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null Strings are 311.\n",
            "The total Stop Words now is 14033 after removing the null strings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = sorted( stop_words, key = len, reverse = True )\n",
        "\n",
        "# removing the links\n",
        "stop_words = [item for item in stop_words if 'http' not in item]\n",
        "stop_words = [item for item in stop_words if 'gov' not in item]\n",
        "\n",
        "stop_words = sorted( stop_words, key = len, reverse = True )\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "mdlmowFrs9tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of stop words are {len(stop_words)}.\\n', stop_words)"
      ],
      "metadata": {
        "id": "TOLtub39zJLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positive and Negative Dictionary"
      ],
      "metadata": {
        "id": "oCBiY-JaIbmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a positive dict\n",
        "positive_file = open('/content/drive/MyDrive/Internship/MasterDictionary/positive-words.txt', 'r', encoding = 'latin')\n",
        "positive_string = positive_file.read()\n",
        "positive_string = positive_string.replace('\\n', ' ')\n",
        "positive_string = positive_string.replace('|', ' ')\n",
        "\n",
        "positive_dict = positive_string.split(' ')\n",
        "positive_dict.remove('')\n",
        "\n",
        "# Making a negative dict\n",
        "negative_file = open('/content/drive/MyDrive/Internship/MasterDictionary/negative-words.txt', 'r', encoding = 'latin')\n",
        "negative_string = negative_file.read()\n",
        "negative_string = negative_string.replace('\\n', ' ')\n",
        "negative_string = negative_string.replace('|', ' ')\n",
        "\n",
        "negative_dict = negative_string.split(' ')\n",
        "negative_dict.remove('')\n",
        "\n",
        "print(positive_dict.count(''), positive_dict)\n",
        "print(negative_dict.count(''), negative_dict)"
      ],
      "metadata": {
        "id": "r1XSRi5GIhq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.read_excel('/content/drive/MyDrive/Internship/Output Data Structure.xlsx')\n",
        "output.head()"
      ],
      "metadata": {
        "id": "pzpHIkYpCyKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web-Scrapping and the Main Script"
      ],
      "metadata": {
        "id": "HFWLrld7H4HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Scrap the tag text from soup\n",
        "def get_data(soup):\n",
        "    try:\n",
        "      title = soup.find('h1').text\n",
        "      \n",
        "      try :\n",
        "        div_element = soup.find_all('div', {'class' : 'tdb-block-inner td-fix-index'})\n",
        "        p_tags = div_element[14].find_all('p')\n",
        "\n",
        "      except Exception as e : # Inconsistent webpage formatting, there are two types of format in the given links\n",
        "        div_element = soup.find_all('div', {'class' : 'td-post-content tagdiv-type'})\n",
        "        p_tags = div_element[0].find_all('p') \n",
        "\n",
        "      content = ''\n",
        "      for p in p_tags:\n",
        "        content += p.text\n",
        "\n",
        "    except Exception as  e:\n",
        "      return e\n",
        "\n",
        "    return title, content\n",
        "\n",
        "# function to count the complex word in a string of text\n",
        "def get_complex(content):\n",
        "  cnt = 0\n",
        "  for wrd in content :\n",
        "    if len(wrd) > 2 : \n",
        "      cnt += 1\n",
        "  return cnt\n",
        "\n",
        "# function to calculate syllabal count handling \"US\"\n",
        "def get_syb(word_list):\n",
        "  vowels = 'aeiou'\n",
        "  cnt = 0\n",
        "  prev_char = None\n",
        "  \n",
        "  for word in word_list:\n",
        "    for char in word:\n",
        "      if char in vowels and (prev_char is None or prev_char not in vowels):\n",
        "        cnt += 1\n",
        "      if word[-2:] == 'es' or word[-2:] == 'ed':\n",
        "        cnt -= 1\n",
        "  return cnt\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "\n",
        "    HEADERS = ({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36', 'Acccept-Language': 'en-US, en; q = 0.5'})\n",
        "\n",
        "    output = pd.read_excel('/content/drive/MyDrive/Internship/Output Data Structure.xlsx')\n",
        "\n",
        "    # Looping over all the URL\n",
        "    for url in output['URL']:\n",
        "        \n",
        "        # hitting the URLs with requests\n",
        "        webpage = requests.get(url, headers = HEADERS)\n",
        "        # parsing the response\n",
        "        soup = BeautifulSoup(webpage.content, 'html.parser')\n",
        "\n",
        "        try:\n",
        "            # get the title and content\n",
        "            title, content = get_data(soup)\n",
        "            \n",
        "            # combine them\n",
        "            content = title + ' ' + content\n",
        "            \n",
        "            # save the original text, will be needed down the line\n",
        "            txt = content\n",
        "            \n",
        "            # save the number of fullstops in the original txt\n",
        "            fullstop_cnt = content.count('.')\n",
        "            \n",
        "            # split the content by space as a delimitor\n",
        "            content = content.split(' ')\n",
        "            \n",
        "            # lowercase cast all the words\n",
        "            content = [word.lower() for word in content]\n",
        "\n",
        "            # cleaning the content with RESPECT TO THE CUSTOM STOPWORDS (as requested) \n",
        "            cleaned_content = list(set(content) - set(stop_words))\n",
        "\n",
        "            # calculating positive score\n",
        "            positive_scr = len(set(content).intersection(positive_dict))\n",
        "            output.loc[output['URL'] == url, 'POSITIVE SCORE'] = positive_scr\n",
        "            \n",
        "            # calculating negative score\n",
        "            negative_scr = len(set(content).intersection(negative_dict))\n",
        "            output.loc[output['URL'] == url, 'NEGATIVE SCORE'] = negative_scr\n",
        "\n",
        "            # calculating polarity score\n",
        "            Polarity_Score = (positive_scr - negative_scr) / ((positive_scr + negative_scr) + 0.000001)\n",
        "            output.loc[output['URL'] == url, 'POLARITY SCORE'] = Polarity_Score\n",
        "\n",
        "            # calculating subjectivity score\n",
        "            subjectivity_score = (positive_scr + negative_scr )/ ( len(cleaned_content) + 0.000001)\n",
        "            output.loc[output['URL'] == url, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
        "\n",
        "            # calculating average sentence length \n",
        "            avg_sentence_len = len(cleaned_content) / fullstop_cnt\n",
        "            output.loc[output['URL'] == url, 'AVG SENTENCE LENGTH'] = avg_sentence_len\n",
        "\n",
        "            # calculating complex word count\n",
        "            complex_word_cnt = get_complex(cleaned_content)\n",
        "            output.loc[output['URL'] == url, 'COMPLEX WORD COUNT'] = complex_word_cnt\n",
        "\n",
        "            # calculating complex word percentage\n",
        "            complex_word_per = (complex_word_cnt / len(cleaned_content)) * 100\n",
        "            output.loc[output['URL'] == url, 'PERCENTAGE OF COMPLEX WORDS'] = complex_word_per\n",
        "            \n",
        "            # calculating fog index\n",
        "            fog_idx = 0.4 * ( avg_sentence_len + complex_word_per )\n",
        "            output.loc[output['URL'] == url, 'FOG INDEX'] = fog_idx\n",
        "\n",
        "            # calculating the words per sentence\n",
        "            words_per_sent = len(cleaned_content) / fullstop_cnt\n",
        "            output.loc[output['URL'] == url, 'AVG NUMBER OF WORDS PER SENTENCE'] = words_per_sent\n",
        "            \n",
        "            \n",
        "            ## processing the stop words from nltk\n",
        "            nltk_stp = set(stopwords.words('english'))\n",
        "            nltk_stp |= set(string.punctuation)\n",
        "            nltk_stp = [word.lower() for word in nltk_stp]\n",
        "            filtered_list = [word for word in cleaned_content if word not in nltk_stp]\n",
        "            word_count = len(filtered_list)\n",
        "            output.loc[output['URL'] == url, 'WORD COUNT'] = word_count\n",
        "\n",
        "            ## syllable count per word\n",
        "            syb_cnt = get_syb(filtered_list) / len(filtered_list)\n",
        "            output.loc[output['URL'] == url, 'SYLLABLE PER WORD'] = syb_cnt\n",
        "\n",
        "            ## Personal Pronouns\n",
        "            count_US = txt.count('US')\n",
        "            txt = txt.lower()\n",
        "            pronoun_cnt = txt.count('I') + txt.count('we') + txt.count('ours') + txt.count('my') + txt.count('us') - 2*count_US\n",
        "            output.loc[output['URL'] == url, 'PERSONAL PRONOUNS'] = pronoun_cnt\n",
        "\n",
        "            ## Average Word Length\n",
        "            avg_word_length = len(txt) / len(cleaned_content)\n",
        "            output.loc[output['URL'] == url, 'AVG WORD LENGTH'] = avg_word_length\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f' The following webpage does not exist for {url}') # 3 instances of webpage not founds or error 404 \n",
        " "
      ],
      "metadata": {
        "id": "YNsoc1R3pfjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final App.py Script"
      ],
      "metadata": {
        "id": "CtUAO3EiwMdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dependencies\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# function for generating all the dicts\n",
        "def get_stop_words(dir):\n",
        "  # initialise a string to save the txt\n",
        "    stop_string = ''\n",
        "\n",
        "    dir_cnt = 0    \n",
        "\n",
        "    # preprocessing and saving the text in the directories in a list named stop_words\n",
        "    for filename in os.listdir(dir) :\n",
        "\n",
        "        f = os.path.join(dir, filename)\n",
        "        with open(f, 'r', encoding = 'latin') as text_file :\n",
        "          dir_cnt += 1\n",
        "\n",
        "          content = text_file.read()\n",
        "          content = content.replace('\\n', ' ')\n",
        "          content = content.replace('|', ' ')\n",
        "\n",
        "          stop_string += content\n",
        "          stop_words = stop_string.split(' ')\n",
        "\n",
        "    # removing any null values\n",
        "    stop_words = [item for item in stop_words if item != '']\n",
        "    stop_words = sorted( stop_words, key = len, reverse = True )\n",
        "\n",
        "    # removing the links\n",
        "    stop_words = [item for item in stop_words if 'http' not in item]\n",
        "    stop_words = [item for item in stop_words if 'gov' not in item]\n",
        "\n",
        "    return stop_words\n",
        "\n",
        "# function for generating positive and negative dicts\n",
        "def get_dicts(pos_path, neg_path):\n",
        "      # Making a positive dict\n",
        "      positive_file = open(pos_path, 'r', encoding = 'latin')\n",
        "      positive_string = positive_file.read()\n",
        "      positive_string = positive_string.replace('\\n', ' ')\n",
        "      positive_string = positive_string.replace('|', ' ')\n",
        "\n",
        "      positive_dict = positive_string.split(' ')\n",
        "      positive_dict.remove('')\n",
        "\n",
        "      # Making a negative dict\n",
        "      negative_file = open(neg_path, 'r', encoding = 'latin')\n",
        "      negative_string = negative_file.read()\n",
        "      negative_string = negative_string.replace('\\n', ' ')\n",
        "      negative_string = negative_string.replace('|', ' ')\n",
        "\n",
        "      negative_dict = negative_string.split(' ')\n",
        "      negative_dict.remove('')\n",
        "\n",
        "      return positive_dict, negative_dict\n",
        "\n",
        "# Function to Scrap the tag text from soup\n",
        "def get_data(soup):\n",
        "    try:\n",
        "      title = soup.find('h1').text\n",
        "      \n",
        "      try :\n",
        "        div_element = soup.find_all('div', {'class' : 'tdb-block-inner td-fix-index'})\n",
        "        p_tags = div_element[14].find_all('p')\n",
        "\n",
        "      except Exception as e : # Inconsistent webpage formatting, there are two types of format in the given links\n",
        "        div_element = soup.find_all('div', {'class' : 'td-post-content tagdiv-type'})\n",
        "        p_tags = div_element[0].find_all('p') \n",
        "\n",
        "      content = ''\n",
        "      for p in p_tags:\n",
        "        content += p.text\n",
        "\n",
        "    except Exception as  e:\n",
        "      print(e)\n",
        "\n",
        "    return title, content\n",
        "\n",
        "# function to count the complex word in a string of text\n",
        "def get_complex(content):\n",
        "  cnt = 0\n",
        "  for wrd in content :\n",
        "    if len(wrd) > 2 : \n",
        "      cnt += 1\n",
        "  return cnt\n",
        "\n",
        "# function to calculate syllabal count, handling \"US\"\n",
        "def get_syb(word_list):\n",
        "  vowels = 'aeiou'\n",
        "  cnt = 0\n",
        "  prev_char = None\n",
        "  \n",
        "  for word in word_list:\n",
        "    for char in word:\n",
        "      if char in vowels and (prev_char is None or prev_char not in vowels):\n",
        "        cnt += 1\n",
        "      if word[-2:] == 'es' or word[-2:] == 'ed':\n",
        "        cnt -= 1\n",
        "  return cnt\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # change your directories here\n",
        "    stop_dir = '/content/drive/MyDrive/Internship/StopWords'\n",
        "    positive_dir =  '/content/drive/MyDrive/Internship/MasterDictionary/positive-words.txt'\n",
        "    negative_dir = '/content/drive/MyDrive/Internship/MasterDictionary/negative-words.txt'\n",
        "\n",
        "    stop_words = get_stop_words(stop_dir)\n",
        "    positive_dict, negative_dict = get_dicts(positive_dir, negative_dir)\n",
        "    HEADERS = ({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36', 'Acccept-Language': 'en-US, en; q = 0.5'})\n",
        "\n",
        "    output_dir = '/content/drive/MyDrive/Internship/Output Data Structure.xlsx'\n",
        "    output = pd.read_excel(output_dir)\n",
        "\n",
        "    # Looping over all the URL\n",
        "    for url in output['URL']:\n",
        "        \n",
        "        # hitting the URLs with requests\n",
        "        webpage = requests.get(url, headers = HEADERS)\n",
        "        # parsing the response\n",
        "        soup = BeautifulSoup(webpage.content, 'html.parser')\n",
        "\n",
        "        try:\n",
        "            # get the title and content\n",
        "            title, content = get_data(soup)\n",
        "            \n",
        "            # combine them\n",
        "            content = title + ' ' + content\n",
        "            \n",
        "            # save the original text, will be needed down the line\n",
        "            txt = content\n",
        "            \n",
        "            # save the number of fullstops in the original txt\n",
        "            fullstop_cnt = content.count('.')\n",
        "            \n",
        "            # split the content by space as a delimitor\n",
        "            content = content.split(' ')\n",
        "            \n",
        "            # lowercase cast all the words\n",
        "            content = [word.lower() for word in content]\n",
        "\n",
        "            # cleaning the content with RESPECT TO THE CUSTOM STOPWORDS (as requested) \n",
        "            cleaned_content = list(set(content) - set(stop_words))\n",
        "\n",
        "            # calculating positive score\n",
        "            positive_scr = len(set(content).intersection(positive_dict))\n",
        "            output.loc[output['URL'] == url, 'POSITIVE SCORE'] = positive_scr\n",
        "            \n",
        "            # calculating negative score\n",
        "            negative_scr = len(set(content).intersection(negative_dict))\n",
        "            output.loc[output['URL'] == url, 'NEGATIVE SCORE'] = negative_scr\n",
        "\n",
        "            # calculating polarity score\n",
        "            Polarity_Score = (positive_scr - negative_scr) / ((positive_scr + negative_scr) + 0.000001)\n",
        "            output.loc[output['URL'] == url, 'POLARITY SCORE'] = Polarity_Score\n",
        "\n",
        "            # calculating subjectivity score\n",
        "            subjectivity_score = (positive_scr + negative_scr )/ ( len(cleaned_content) + 0.000001)\n",
        "            output.loc[output['URL'] == url, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
        "\n",
        "            # calculating average sentence length \n",
        "            avg_sentence_len = len(cleaned_content) / fullstop_cnt\n",
        "            output.loc[output['URL'] == url, 'AVG SENTENCE LENGTH'] = avg_sentence_len\n",
        "\n",
        "            # calculating complex word count\n",
        "            complex_word_cnt = get_complex(cleaned_content)\n",
        "            output.loc[output['URL'] == url, 'COMPLEX WORD COUNT'] = complex_word_cnt\n",
        "\n",
        "            # calculating complex word percentage\n",
        "            complex_word_per = (complex_word_cnt / len(cleaned_content)) * 100\n",
        "            output.loc[output['URL'] == url, 'PERCENTAGE OF COMPLEX WORDS'] = complex_word_per\n",
        "            \n",
        "            # calculating fog index\n",
        "            fog_idx = 0.4 * ( avg_sentence_len + complex_word_per )\n",
        "            output.loc[output['URL'] == url, 'FOG INDEX'] = fog_idx\n",
        "\n",
        "            # calculating the words per sentence\n",
        "            words_per_sent = len(cleaned_content) / fullstop_cnt\n",
        "            output.loc[output['URL'] == url, 'AVG NUMBER OF WORDS PER SENTENCE'] = words_per_sent\n",
        "            \n",
        "            \n",
        "            ## processing the stop words from nltk\n",
        "            nltk_stp = set(stopwords.words('english'))\n",
        "            nltk_stp |= set(string.punctuation)\n",
        "            nltk_stp = [word.lower() for word in nltk_stp]\n",
        "            filtered_list = [word for word in cleaned_content if word not in nltk_stp]\n",
        "            word_count = len(filtered_list)\n",
        "            output.loc[output['URL'] == url, 'WORD COUNT'] = word_count\n",
        "\n",
        "            ## syllable count per word\n",
        "            syb_cnt = get_syb(filtered_list) / len(filtered_list)\n",
        "            output.loc[output['URL'] == url, 'SYLLABLE PER WORD'] = syb_cnt\n",
        "\n",
        "            ## Personal Pronouns\n",
        "            count_US = txt.count('US')\n",
        "            txt = txt.lower()\n",
        "            pronoun_cnt = txt.count('I') + txt.count('we') + txt.count('ours') + txt.count('my') + txt.count('us') - 2*count_US\n",
        "            output.loc[output['URL'] == url, 'PERSONAL PRONOUNS'] = pronoun_cnt\n",
        "\n",
        "            ## Average Word Length\n",
        "            avg_word_length = len(txt) / len(cleaned_content)\n",
        "            output.loc[output['URL'] == url, 'AVG WORD LENGTH'] = avg_word_length\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f' The following webpage does not exist for {url}') # 3 instances of webpage not founds or error 404 \n",
        "    # save the output as a excel file ( as requested )\n",
        "    output.to_excel('Output Data Structure.xlsx', index = False)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3z_Rd4IUgI5",
        "outputId": "65816f7b-4d8e-4fec-b0dc-01c44381d551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'NoneType' object has no attribute 'text'\n",
            " The following webpage does not exist for https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "'NoneType' object has no attribute 'text'\n",
            " The following webpage does not exist for https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "'NoneType' object has no attribute 'text'\n",
            " The following webpage does not exist for https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
          ]
        }
      ]
    }
  ]
}